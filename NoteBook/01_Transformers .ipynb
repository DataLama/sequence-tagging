{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [가제] Transformers의 데이터파이프라인\n",
    "\n",
    "최근 Huggingface(🤗)의 transformers가 NLP의 democratization을 이끌고 있다. transformers는 SoTA 모델을 가장 쉽고 빠르게 모든 사람이 쓸 수 있도록 만들어졌다. 하지만, transformers의 간단함의 감동을 하고, 막상 실제 문제를 풀려고 하면, 복잡하고 추상화된 코드 구조 때문에 어려움을 겪는 경우가 많다.(물론 버전이 올라가면서 코드가 엄청나가 간소화되고 있다.)\n",
    "\n",
    "특히, 가장 고민해야되는 부분이 데이터파이프라인이다. processor, convert_examples_to_features를 처음 보면 뭐지?라는 생각이 든다. 이에 대한 정리를 하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformers의 데이터 파이프라인 이해하기\n",
    "허깅페이스의 transformers의 코드를 보면, PLM(Pretrained Language Model)을 학습할 때, 어떤 부분을 사람들이 고민해왔는지 간접적으로 이해할 수 있습니다. 특히, RNN 계열의 모델링을 해왔던 사람들에게 transformers의 `Dataset`은 처음 보기엔 생소하고 너무 복잡할 수 있습니다. 하지만 실제 코드를 하나씩 뜯어보면 PLM을 학습할 때, 데이터 파이프라인 측면에서 어떤 점을 고민하는지 이해할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input 데이터에 대한 표준화\n",
    "transformers는 데이터 파이프라인을 표준화하기 위해서 `InputExample`과 `InputFeatures`라는 데이터클래스를 정의했습니다. 코드를 살펴보면 아래와 같습니다.\n",
    "\n",
    "- **`InputExample`**\n",
    "    - Raw 데이터를 다루는 역할을 합니다.\n",
    "    - 일반적으로 text, token classification 문제와 같이 single sequence 데이터에 대한 문제를 해결할 경우 `text_a`만 사용합니다.\n",
    "    - 반면, STS, SQuad와 같이 pair형태의 sequence에 대한 문제를 해결할 경우, `text_a`와 `text_b`를 함께 사용합니다.\n",
    "    \n",
    "```python\n",
    "@dataclass\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    A single training/test example for simple sequence classification.\n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "        text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "        label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "\n",
    "    guid: str\n",
    "    text_a: str\n",
    "    text_b: Optional[str] = None\n",
    "    label: Optional[str] = None\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\"\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- **`InputFeatures`**\n",
    "    - Feature화된 데이터를 다루는 역할을 합니다.\n",
    "        - NLP에서 가장 대표적인 Feature화는 tokenization입니다.\n",
    "\n",
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class InputFeatures:\n",
    "    \"\"\"\n",
    "    A single set of features of data.\n",
    "    Property names are the same names as the corresponding inputs to a model.\n",
    "    Args:\n",
    "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
    "            Mask values selected in ``[0, 1]``:\n",
    "            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n",
    "        token_type_ids: (Optional) Segment token indices to indicate first and second\n",
    "            portions of the inputs. Only some models use them.\n",
    "        label: (Optional) Label corresponding to the input. Int for classification problems,\n",
    "            float for regression problems.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids: List[int]\n",
    "    attention_mask: Optional[List[int]] = None\n",
    "    token_type_ids: Optional[List[int]] = None\n",
    "    label: Optional[Union[int, float]] = None\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self)) + \"\\n\"\n",
    "```\n",
    "\n",
    "위 코드를 살펴보면, 공통적으로 `to_json_string`이라는 메서드가 존재함을 확인할 수 있습니다. 이는 tokenization을 적용한 데이터를 캐싱하여 사용하기 위함입니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    A single training/test example for simple sequence classification.\n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "        text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "        label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "\n",
    "    guid: str\n",
    "    text_a: str\n",
    "    text_b: Optional[str] = None\n",
    "    label: Optional[str] = None\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `InputExample` 와 `InputFeatures`\n",
    "`InputExample`과 `InputFeatures`는 PLM의 input 데이터를 다루는데 특화된 dataclass임.\n",
    "\n",
    "- `InputExample`은 PLM과 관련된 raw 형태의 데이터를 다루는데 적합함.\n",
    "    - Pair 형태의 데이터를 받는 것이 일반적\n",
    "    - Pa\n",
    "    - 많은 PLM이 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "- `InputFeatures`은 InputExample에서 Feature화된 형태의 데이터임.\n",
    "    - NLP에서 Feature화란 tokenization과 index화를 통해 PLM 모델의 input 형태로 변환해주는 것임.\n",
    "    - 또한, attention_mask와 Segments와 관련된 ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors.utils import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`InputExample`, `InputFeatures`, `DataProcessor`\n",
    "\n",
    "\n",
    "<!-- ### 주요특징\n",
    "- `processor`라는 객체를 통해 data를 read함.\n",
    "- `***_convert_examples_to_features`라는 함수를 통해 raw_data를 feature화함.\n",
    "    - NLP에서 feature화라는 것은 tokenization과 거의 동치임.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InputExample, InputFeatures, DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlueDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    args: GlueDataTrainingArguments\n",
    "    output_mode: str\n",
    "    features: List[InputFeatures]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: GlueDataTrainingArguments,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        limit_length: Optional[int] = None,\n",
    "        mode: Union[str, Split] = Split.train,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.processor = glue_processors[args.task_name]()\n",
    "        self.output_mode = glue_output_modes[args.task_name]\n",
    "        if isinstance(mode, str):\n",
    "            try:\n",
    "                mode = Split[mode]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"mode is not a valid split name\")\n",
    "        # Load data features from cache or dataset file\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else args.data_dir,\n",
    "            \"cached_{}_{}_{}_{}\".format(\n",
    "                mode.value, tokenizer.__class__.__name__, str(args.max_seq_length), args.task_name,\n",
    "            ),\n",
    "        )\n",
    "        label_list = self.processor.get_labels()\n",
    "        if args.task_name in [\"mnli\", \"mnli-mm\"] and tokenizer.__class__ in (\n",
    "            RobertaTokenizer,\n",
    "            RobertaTokenizerFast,\n",
    "            XLMRobertaTokenizer,\n",
    "            BartTokenizer,\n",
    "            BartTokenizerFast,\n",
    "        ):\n",
    "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
    "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
    "        self.label_list = label_list\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "                start = time.time()\n",
    "                self.features = torch.load(cached_features_file)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {args.data_dir}\")\n",
    "\n",
    "                if mode == Split.dev:\n",
    "                    examples = self.processor.get_dev_examples(args.data_dir)\n",
    "                elif mode == Split.test:\n",
    "                    examples = self.processor.get_test_examples(args.data_dir)\n",
    "                else:\n",
    "                    examples = self.processor.get_train_examples(args.data_dir)\n",
    "                if limit_length is not None:\n",
    "                    examples = examples[:limit_length]\n",
    "                self.features = glue_convert_examples_to_features(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    max_length=args.max_seq_length,\n",
    "                    label_list=label_list,\n",
    "                    output_mode=self.output_mode,\n",
    "                )\n",
    "                start = time.time()\n",
    "                torch.save(self.features, cached_features_file)\n",
    "                # ^ This seems to take a lot of time so I want to investigate why and how we can improve.\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.label_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
