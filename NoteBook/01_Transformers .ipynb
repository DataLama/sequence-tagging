{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ê°€ì œ] Transformersì˜ ë°ì´í„°íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ìµœê·¼ Huggingface(ğŸ¤—)ì˜ transformersê°€ NLPì˜ democratizationì„ ì´ëŒê³  ìˆë‹¤. transformersëŠ” SoTA ëª¨ë¸ì„ ê°€ì¥ ì‰½ê³  ë¹ ë¥´ê²Œ ëª¨ë“  ì‚¬ëŒì´ ì“¸ ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ì¡Œë‹¤. í•˜ì§€ë§Œ, transformersì˜ ê°„ë‹¨í•¨ì˜ ê°ë™ì„ í•˜ê³ , ë§‰ìƒ ì‹¤ì œ ë¬¸ì œë¥¼ í’€ë ¤ê³  í•˜ë©´, ë³µì¡í•˜ê³  ì¶”ìƒí™”ëœ ì½”ë“œ êµ¬ì¡° ë•Œë¬¸ì— ì–´ë ¤ì›€ì„ ê²ªëŠ” ê²½ìš°ê°€ ë§ë‹¤.(ë¬¼ë¡  ë²„ì „ì´ ì˜¬ë¼ê°€ë©´ì„œ ì½”ë“œê°€ ì—„ì²­ë‚˜ê°€ ê°„ì†Œí™”ë˜ê³  ìˆë‹¤.)\n",
    "\n",
    "íŠ¹íˆ, ê°€ì¥ ê³ ë¯¼í•´ì•¼ë˜ëŠ” ë¶€ë¶„ì´ ë°ì´í„°íŒŒì´í”„ë¼ì¸ì´ë‹¤. processor, convert_examples_to_featuresë¥¼ ì²˜ìŒ ë³´ë©´ ë­ì§€?ë¼ëŠ” ìƒê°ì´ ë“ ë‹¤. ì´ì— ëŒ€í•œ ì •ë¦¬ë¥¼ í•˜ê² ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformersì˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì´í•´í•˜ê¸°\n",
    "í—ˆê¹…í˜ì´ìŠ¤ì˜ transformersì˜ ì½”ë“œë¥¼ ë³´ë©´, PLM(Pretrained Language Model)ì„ í•™ìŠµí•  ë•Œ, ì–´ë–¤ ë¶€ë¶„ì„ ì‚¬ëŒë“¤ì´ ê³ ë¯¼í•´ì™”ëŠ”ì§€ ê°„ì ‘ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, RNN ê³„ì—´ì˜ ëª¨ë¸ë§ì„ í•´ì™”ë˜ ì‚¬ëŒë“¤ì—ê²Œ transformersì˜ `Dataset`ì€ ì²˜ìŒ ë³´ê¸°ì—” ìƒì†Œí•˜ê³  ë„ˆë¬´ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ ì½”ë“œë¥¼ í•˜ë‚˜ì”© ëœ¯ì–´ë³´ë©´ PLMì„ í•™ìŠµí•  ë•Œ, ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì¸¡ë©´ì—ì„œ ì–´ë–¤ ì ì„ ê³ ë¯¼í•˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input ë°ì´í„°ì— ëŒ€í•œ í‘œì¤€í™”\n",
    "transformersëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ í‘œì¤€í™”í•˜ê¸° ìœ„í•´ì„œ `InputExample`ê³¼ `InputFeatures`ë¼ëŠ” ë°ì´í„°í´ë˜ìŠ¤ë¥¼ ì •ì˜í–ˆìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ ì‚´í´ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- **`InputExample`**\n",
    "    - Raw ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "    - ì¼ë°˜ì ìœ¼ë¡œ text, token classification ë¬¸ì œì™€ ê°™ì´ single sequence ë°ì´í„°ì— ëŒ€í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ê²½ìš° `text_a`ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    - ë°˜ë©´, STS, SQuadì™€ ê°™ì´ pairí˜•íƒœì˜ sequenceì— ëŒ€í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ê²½ìš°, `text_a`ì™€ `text_b`ë¥¼ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \n",
    "```python\n",
    "@dataclass\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    A single training/test example for simple sequence classification.\n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "        text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "        label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "\n",
    "    guid: str\n",
    "    text_a: str\n",
    "    text_b: Optional[str] = None\n",
    "    label: Optional[str] = None\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\"\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- **`InputFeatures`**\n",
    "    - Featureí™”ëœ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "        - NLPì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ Featureí™”ëŠ” tokenizationì…ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class InputFeatures:\n",
    "    \"\"\"\n",
    "    A single set of features of data.\n",
    "    Property names are the same names as the corresponding inputs to a model.\n",
    "    Args:\n",
    "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
    "            Mask values selected in ``[0, 1]``:\n",
    "            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n",
    "        token_type_ids: (Optional) Segment token indices to indicate first and second\n",
    "            portions of the inputs. Only some models use them.\n",
    "        label: (Optional) Label corresponding to the input. Int for classification problems,\n",
    "            float for regression problems.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids: List[int]\n",
    "    attention_mask: Optional[List[int]] = None\n",
    "    token_type_ids: Optional[List[int]] = None\n",
    "    label: Optional[Union[int, float]] = None\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self)) + \"\\n\"\n",
    "```\n",
    "\n",
    "ìœ„ ì½”ë“œë¥¼ ì‚´í´ë³´ë©´, ê³µí†µì ìœ¼ë¡œ `to_json_string`ì´ë¼ëŠ” ë©”ì„œë“œê°€ ì¡´ì¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” tokenizationì„ ì ìš©í•œ ë°ì´í„°ë¥¼ ìºì‹±í•˜ì—¬ ì‚¬ìš©í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    A single training/test example for simple sequence classification.\n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "        text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "        label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "\n",
    "    guid: str\n",
    "    text_a: str\n",
    "    text_b: Optional[str] = None\n",
    "    label: Optional[str] = None\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `InputExample` ì™€ `InputFeatures`\n",
    "`InputExample`ê³¼ `InputFeatures`ëŠ” PLMì˜ input ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ”ë° íŠ¹í™”ëœ dataclassì„.\n",
    "\n",
    "- `InputExample`ì€ PLMê³¼ ê´€ë ¨ëœ raw í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ”ë° ì í•©í•¨.\n",
    "    - Pair í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë°›ëŠ” ê²ƒì´ ì¼ë°˜ì \n",
    "    - Pa\n",
    "    - ë§ì€ PLMì´ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "- `InputFeatures`ì€ InputExampleì—ì„œ Featureí™”ëœ í˜•íƒœì˜ ë°ì´í„°ì„.\n",
    "    - NLPì—ì„œ Featureí™”ë€ tokenizationê³¼ indexí™”ë¥¼ í†µí•´ PLM ëª¨ë¸ì˜ input í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ëŠ” ê²ƒì„.\n",
    "    - ë˜í•œ, attention_maskì™€ Segmentsì™€ ê´€ë ¨ëœ ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors.utils import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`InputExample`, `InputFeatures`, `DataProcessor`\n",
    "\n",
    "\n",
    "<!-- ### ì£¼ìš”íŠ¹ì§•\n",
    "- `processor`ë¼ëŠ” ê°ì²´ë¥¼ í†µí•´ dataë¥¼ readí•¨.\n",
    "- `***_convert_examples_to_features`ë¼ëŠ” í•¨ìˆ˜ë¥¼ í†µí•´ raw_dataë¥¼ featureí™”í•¨.\n",
    "    - NLPì—ì„œ featureí™”ë¼ëŠ” ê²ƒì€ tokenizationê³¼ ê±°ì˜ ë™ì¹˜ì„.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InputExample, InputFeatures, DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlueDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    args: GlueDataTrainingArguments\n",
    "    output_mode: str\n",
    "    features: List[InputFeatures]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: GlueDataTrainingArguments,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        limit_length: Optional[int] = None,\n",
    "        mode: Union[str, Split] = Split.train,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.processor = glue_processors[args.task_name]()\n",
    "        self.output_mode = glue_output_modes[args.task_name]\n",
    "        if isinstance(mode, str):\n",
    "            try:\n",
    "                mode = Split[mode]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"mode is not a valid split name\")\n",
    "        # Load data features from cache or dataset file\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else args.data_dir,\n",
    "            \"cached_{}_{}_{}_{}\".format(\n",
    "                mode.value, tokenizer.__class__.__name__, str(args.max_seq_length), args.task_name,\n",
    "            ),\n",
    "        )\n",
    "        label_list = self.processor.get_labels()\n",
    "        if args.task_name in [\"mnli\", \"mnli-mm\"] and tokenizer.__class__ in (\n",
    "            RobertaTokenizer,\n",
    "            RobertaTokenizerFast,\n",
    "            XLMRobertaTokenizer,\n",
    "            BartTokenizer,\n",
    "            BartTokenizerFast,\n",
    "        ):\n",
    "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
    "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
    "        self.label_list = label_list\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "                start = time.time()\n",
    "                self.features = torch.load(cached_features_file)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {args.data_dir}\")\n",
    "\n",
    "                if mode == Split.dev:\n",
    "                    examples = self.processor.get_dev_examples(args.data_dir)\n",
    "                elif mode == Split.test:\n",
    "                    examples = self.processor.get_test_examples(args.data_dir)\n",
    "                else:\n",
    "                    examples = self.processor.get_train_examples(args.data_dir)\n",
    "                if limit_length is not None:\n",
    "                    examples = examples[:limit_length]\n",
    "                self.features = glue_convert_examples_to_features(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    max_length=args.max_seq_length,\n",
    "                    label_list=label_list,\n",
    "                    output_mode=self.output_mode,\n",
    "                )\n",
    "                start = time.time()\n",
    "                torch.save(self.features, cached_features_file)\n",
    "                # ^ This seems to take a lot of time so I want to investigate why and how we can improve.\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.label_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
