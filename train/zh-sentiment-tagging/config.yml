authorName: datalama
experimentName: zh_sentiment_tagging
trialConcurrency: 8
maxExecDuration: 36h
maxTrialNum: 200
#choice: local, remote, pai
trainingServicePlatform: local
searchSpacePath: search_space.json
#choice: true, false
useAnnotation: false
logDir: /root/sequence-tagging/train/experiments
tuner:
  #choice: TPE, Random, Anneal, Evolution, BatchTuner, MetisTuner
  #SMAC (SMAC should be installed through nnictl)
  builtinTunerName: TPE
  classArgs:
    #choice: maximize, minimize
    optimize_mode: maximize
  gpuIndices: "0,1,2,3,4,5,6,7"
# assessor:
#     builtinAssessorName: Medianstop
#     classArgs:
#       optimize_mode: maximize
#       start_step: 15
trial:
  command: python trainer.py --model_name_or_path 'hfl/chinese-electra-180g-small-discriminator' \
                                --task_type "SentimentTagging" \
                                --data_dir "data/step-03" \
                                --output_dir "ckpt/step-03" \
                                --overwrite_output_dir \
                                --num_train_epochs "3" \
                                --evaluation_strategy "steps" \
                                --evaluate_during_training \
                                --logging_first_step \
                                --do_train \
                                --do_eval \
                                --do_predict\
                                --per_device_train_batch_size 4 \
                                --per_device_eval_batch_size 4 \
                                --gradient_accumulation_steps 8 \
                                --metric_for_best_model "token-f1"\
                                --greater_is_better True
                                
  codeDir: .
  gpuNum: 1
#localConfig:
#  maxTrialNumPerGpu:  2