{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- callback으로 logging 다루기.\n",
    "- prediction\n",
    "- model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  FutureWarning,\n",
      "PyTorch: setting up devices\n",
      "11/12/2020 21:57:23 - WARNING - trainer -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/12/2020 21:57:23 - INFO - trainer -   Training/evaluation parameters TrainingArguments(output_dir='ckpt/test', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov12_21-57-23_62425c79d67a', logging_first_step=True, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='ckpt/test', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
      "loading configuration file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/config.json from cache at /root/.cache/torch/transformers/724e1f668050592b5e7c58a28126ec13787a9298186a6a24dae50fedfcbfcf61.179ddff154edb6b86593853c963eaeefb8b7ce361896fa7669565f9ae94cf910\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"\\uae0d\\uc815-B\",\n",
      "    \"2\": \"\\uae0d\\uc815-I\",\n",
      "    \"3\": \"\\ubd80\\uc815-B\",\n",
      "    \"4\": \"\\ubd80\\uc815-I\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"O\": 0,\n",
      "    \"\\uae0d\\uc815-B\": 1,\n",
      "    \"\\uae0d\\uc815-I\": 2,\n",
      "    \"\\ubd80\\uc815-B\": 3,\n",
      "    \"\\ubd80\\uc815-I\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/config.json from cache at /root/.cache/torch/transformers/724e1f668050592b5e7c58a28126ec13787a9298186a6a24dae50fedfcbfcf61.179ddff154edb6b86593853c963eaeefb8b7ce361896fa7669565f9ae94cf910\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Model name 'hfl/chinese-electra-180g-small-discriminator' not found in model shortcut name list (google/electra-small-generator, google/electra-base-generator, google/electra-large-generator, google/electra-small-discriminator, google/electra-base-discriminator, google/electra-large-discriminator). Assuming 'hfl/chinese-electra-180g-small-discriminator' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/878059f9a3ac7a89cb5a7c688bab08b96d77030f03998d05e10fa1ed879a4644.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/added_tokens.json from cache at /root/.cache/torch/transformers/da3aaf6286a52a0d036c92ee2f27c7c10346f87aae054e74b000c15107a60524.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/special_tokens_map.json from cache at /root/.cache/torch/transformers/ad341bd9be1c00235337f8b298b942e75814ee044f5368f3f0a409ed6e615a12.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/tokenizer_config.json from cache at /root/.cache/torch/transformers/04b1187c7b41cc9a563e2b39d8d9ab070e87ffc06c3e11da4ee22c4592de834b.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/tokenizer.json from cache at None\n",
      "loading weights file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/7ce25b53bc086ba98188d0fdb53443830f2c5d328997a2ca3de6b4d82eaeb602.fdc2b7144880684ec9e6735ddfce1b84609c4e34df40e6b218adee95967bc92f\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b645c5a2da2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMAIN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/sequence-tagging/train/zh-sentiment-tagging/trainer.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;31m# load pretrained model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         config = AutoConfig.from_pretrained(\n\u001b[1;32m    204\u001b[0m         \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_name\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             return MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING[type(config)].from_pretrained(\n\u001b[0;32m-> 1426\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m             )\n\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_electra.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melectra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_electra.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_project\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_electra.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mElectraLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     def forward(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_electra.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mElectraLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     def forward(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_electra.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_cross_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_cross_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_electra.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraSelfOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruned_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_electra.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_probs_dropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAIN().main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  FutureWarning,\n",
      "11/12/2020 21:09:31 - WARNING - trainer -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/12/2020 21:09:31 - INFO - trainer -   Training/evaluation parameters TrainingArguments(output_dir='ckpt/test', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov12_21-09-31_62425c79d67a', logging_first_step=True, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='ckpt/test', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n"
     ]
    }
   ],
   "source": [
    "test = MAIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/config.json from cache at /root/.cache/torch/transformers/724e1f668050592b5e7c58a28126ec13787a9298186a6a24dae50fedfcbfcf61.179ddff154edb6b86593853c963eaeefb8b7ce361896fa7669565f9ae94cf910\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"\\uae0d\\uc815-B\",\n",
      "    \"2\": \"\\uae0d\\uc815-I\",\n",
      "    \"3\": \"\\ubd80\\uc815-B\",\n",
      "    \"4\": \"\\ubd80\\uc815-I\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"O\": 0,\n",
      "    \"\\uae0d\\uc815-B\": 1,\n",
      "    \"\\uae0d\\uc815-I\": 2,\n",
      "    \"\\ubd80\\uc815-B\": 3,\n",
      "    \"\\ubd80\\uc815-I\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/config.json from cache at /root/.cache/torch/transformers/724e1f668050592b5e7c58a28126ec13787a9298186a6a24dae50fedfcbfcf61.179ddff154edb6b86593853c963eaeefb8b7ce361896fa7669565f9ae94cf910\n",
      "Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Model name 'hfl/chinese-electra-180g-small-discriminator' not found in model shortcut name list (google/electra-small-generator, google/electra-base-generator, google/electra-large-generator, google/electra-small-discriminator, google/electra-base-discriminator, google/electra-large-discriminator). Assuming 'hfl/chinese-electra-180g-small-discriminator' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/878059f9a3ac7a89cb5a7c688bab08b96d77030f03998d05e10fa1ed879a4644.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/added_tokens.json from cache at /root/.cache/torch/transformers/da3aaf6286a52a0d036c92ee2f27c7c10346f87aae054e74b000c15107a60524.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/special_tokens_map.json from cache at /root/.cache/torch/transformers/ad341bd9be1c00235337f8b298b942e75814ee044f5368f3f0a409ed6e615a12.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/tokenizer_config.json from cache at /root/.cache/torch/transformers/04b1187c7b41cc9a563e2b39d8d9ab070e87ffc06c3e11da4ee22c4592de834b.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562\n",
      "loading file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/tokenizer.json from cache at None\n",
      "loading weights file https://huggingface.co/hfl/chinese-electra-180g-small-discriminator/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/7ce25b53bc086ba98188d0fdb53443830f2c5d328997a2ca3de6b4d82eaeb602.fdc2b7144880684ec9e6735ddfce1b84609c4e34df40e6b218adee95967bc92f\n",
      "Some weights of the model checkpoint at hfl/chinese-electra-180g-small-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at hfl/chinese-electra-180g-small-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/12/2020 21:09:41 - INFO - filelock -   Lock 140405904508560 acquired on data/step-00/basic/cached_train_ElectraTokenizer_128.lock\n",
      "11/12/2020 21:09:41 - INFO - finetune.data_utils_ner -   Loading features from cached file data/step-00/basic/cached_train_ElectraTokenizer_128\n",
      "11/12/2020 21:09:42 - INFO - filelock -   Lock 140405904508560 released on data/step-00/basic/cached_train_ElectraTokenizer_128.lock\n",
      "11/12/2020 21:09:42 - INFO - filelock -   Lock 140405904972688 acquired on data/step-00/basic/cached_dev_ElectraTokenizer_128.lock\n",
      "11/12/2020 21:09:42 - INFO - finetune.data_utils_ner -   Loading features from cached file data/step-00/basic/cached_dev_ElectraTokenizer_128\n",
      "11/12/2020 21:09:42 - INFO - filelock -   Lock 140405904972688 released on data/step-00/basic/cached_dev_ElectraTokenizer_128.lock\n",
      "***** Running training *****\n",
      "  Num examples = 5578\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 525\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [525/525 01:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Token-f1</th>\n",
       "      <th>Token-acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.796700</td>\n",
       "      <td>0.641433</td>\n",
       "      <td>0.538061</td>\n",
       "      <td>0.768780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.587734</td>\n",
       "      <td>0.553932</td>\n",
       "      <td>0.703588</td>\n",
       "      <td>0.790795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.517447</td>\n",
       "      <td>0.524957</td>\n",
       "      <td>0.710056</td>\n",
       "      <td>0.800857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1395\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ckpt/test/checkpoint-175\n",
      "Configuration saved in ckpt/test/checkpoint-175/config.json\n",
      "Model weights saved in ckpt/test/checkpoint-175/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5380614761613114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/test/checkpoint-350\n",
      "Configuration saved in ckpt/test/checkpoint-350/config.json\n",
      "Model weights saved in ckpt/test/checkpoint-350/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7035882906149066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/test/checkpoint-525\n",
      "Configuration saved in ckpt/test/checkpoint-525/config.json\n",
      "Model weights saved in ckpt/test/checkpoint-525/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7100557109445399\n",
      "save~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/test\n",
      "Configuration saved in ckpt/test/config.json\n",
      "Model weights saved in ckpt/test/pytorch_model.bin\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1174: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
      "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
      "11/12/2020 21:10:51 - INFO - trainer -   *** Evaluate ***\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1395\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 21:10:57 - INFO - trainer -   ***** Eval results *****\n",
      "11/12/2020 21:10:57 - INFO - trainer -     eval_loss = 0.5249573588371277\n",
      "11/12/2020 21:10:57 - INFO - trainer -     eval_token-f1 = 0.7100557109445399\n",
      "11/12/2020 21:10:57 - INFO - trainer -     eval_token-acc = 0.8008571763405936\n",
      "11/12/2020 21:10:57 - INFO - trainer -     epoch = 3.0\n"
     ]
    }
   ],
   "source": [
    "test.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 if metric else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='tmp', metric_for_best_model='token-f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'token-f1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.metric_for_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use in conjunction with :obj:`load_best_model_at_end` to specify the metric to use to compare two different\n",
    " |          models. Must be the name of a metric returned by the evaluation with or without the prefix :obj:`\"eval_\"`.\n",
    " |          Will default to :obj:`\"loss\"` if unspecified and :obj:`load_best_model_at_end=True` (to use the evaluation\n",
    " |          loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TrainingArguments in module transformers.training_args:\n",
      "\n",
      "class TrainingArguments(builtins.object)\n",
      " |  TrainingArguments(output_dir: str, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = None, do_predict: bool = False, evaluate_during_training: bool = False, evaluation_strategy: transformers.trainer_utils.EvaluationStrategy = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Union[int, NoneType] = None, per_gpu_eval_batch_size: Union[int, NoneType] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Union[int, NoneType] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, warmup_steps: int = 0, logging_dir: Union[str, NoneType] = <factory>, logging_first_step: bool = False, logging_steps: int = 500, save_steps: int = 500, save_total_limit: Union[int, NoneType] = None, no_cuda: bool = False, seed: int = 42, fp16: bool = False, fp16_opt_level: str = 'O1', local_rank: int = -1, tpu_num_cores: Union[int, NoneType] = None, tpu_metrics_debug: bool = False, debug: bool = False, dataloader_drop_last: bool = False, eval_steps: int = None, dataloader_num_workers: int = 0, past_index: int = -1, run_name: Union[str, NoneType] = None, disable_tqdm: Union[bool, NoneType] = None, remove_unused_columns: Union[bool, NoneType] = True, label_names: Union[List[str], NoneType] = None, load_best_model_at_end: Union[bool, NoneType] = False, metric_for_best_model: Union[str, NoneType] = None, greater_is_better: Union[bool, NoneType] = None) -> None\n",
      " |  \n",
      " |  TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
      " |  itself**.\n",
      " |  \n",
      " |  Using :class:`~transformers.HfArgumentParser` we can turn this class into argparse arguments to be able to specify\n",
      " |  them on the command line.\n",
      " |  \n",
      " |  Parameters:\n",
      " |      output_dir (:obj:`str`):\n",
      " |          The output directory where the model predictions and checkpoints will be written.\n",
      " |      overwrite_output_dir (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          If :obj:`True`, overwrite the content of the output directory. Use this to continue training if\n",
      " |          :obj:`output_dir` points to a checkpoint directory.\n",
      " |      do_train (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Whether to run training or not. This argument is not directly used by :class:`~transformers.Trainer`, it's\n",
      " |          intended to be used by your training/evaluation scripts instead. See the `example scripts\n",
      " |          <https://github.com/huggingface/transformers/tree/master/examples>`__ for more details.\n",
      " |      do_eval (:obj:`bool`, `optional`):\n",
      " |          Whether to run evaluation on the dev set or not. Will be set to :obj:`True` if :obj:`evaluation_strategy`\n",
      " |          is different from :obj:`\"no\"`. This argument is not directly used by :class:`~transformers.Trainer`, it's\n",
      " |          intended to be used by your training/evaluation scripts instead. See the `example scripts\n",
      " |          <https://github.com/huggingface/transformers/tree/master/examples>`__ for more details.\n",
      " |      do_predict (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Whether to run predictions on the test set or not. This argument is not directly used by\n",
      " |          :class:`~transformers.Trainer`, it's intended to be used by your training/evaluation scripts instead. See\n",
      " |          the `example scripts <https://github.com/huggingface/transformers/tree/master/examples>`__ for more\n",
      " |          details.\n",
      " |      evaluation_strategy (:obj:`str` or :class:`~transformers.trainer_utils.EvaluationStrategy`, `optional`, defaults to :obj:`\"no\"`):\n",
      " |          The evaluation strategy to adopt during training. Possible values are:\n",
      " |  \n",
      " |              * :obj:`\"no\"`: No evaluation is done during training.\n",
      " |              * :obj:`\"steps\"`: Evaluation is done (and logged) every :obj:`eval_steps`.\n",
      " |              * :obj:`\"epoch\"`: Evaluation is done at the end of each epoch.\n",
      " |  \n",
      " |      prediction_loss_only (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |          When performing evaluation and predictions, only returns the loss.\n",
      " |      per_device_train_batch_size (:obj:`int`, `optional`, defaults to 8):\n",
      " |          The batch size per GPU/TPU core/CPU for training.\n",
      " |      per_device_eval_batch_size (:obj:`int`, `optional`, defaults to 8):\n",
      " |          The batch size per GPU/TPU core/CPU for evaluation.\n",
      " |      gradient_accumulation_steps (:obj:`int`, `optional`, defaults to 1):\n",
      " |          Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
      " |  \n",
      " |          .. warning::\n",
      " |  \n",
      " |              When using gradient accumulation, one step is counted as one step with backward pass. Therefore,\n",
      " |              logging, evaluation, save will be conducted every ``gradient_accumulation_steps * xxx_step`` training\n",
      " |              examples.\n",
      " |      eval_accumulation_steps (:obj:`int`, `optional`):\n",
      " |          Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n",
      " |          left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but\n",
      " |          requires more memory).\n",
      " |      learning_rate (:obj:`float`, `optional`, defaults to 5e-5):\n",
      " |          The initial learning rate for Adam.\n",
      " |      weight_decay (:obj:`float`, `optional`, defaults to 0):\n",
      " |          The weight decay to apply (if not zero).\n",
      " |      adam_epsilon (:obj:`float`, `optional`, defaults to 1e-8):\n",
      " |          Epsilon for the Adam optimizer.\n",
      " |      max_grad_norm (:obj:`float`, `optional`, defaults to 1.0):\n",
      " |          Maximum gradient norm (for gradient clipping).\n",
      " |      num_train_epochs(:obj:`float`, `optional`, defaults to 3.0):\n",
      " |          Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n",
      " |          the last epoch before stopping training).\n",
      " |      max_steps (:obj:`int`, `optional`, defaults to -1):\n",
      " |          If set to a positive number, the total number of training steps to perform. Overrides\n",
      " |          :obj:`num_train_epochs`.\n",
      " |      warmup_steps (:obj:`int`, `optional`, defaults to 0):\n",
      " |          Number of steps used for a linear warmup from 0 to :obj:`learning_rate`.\n",
      " |      logging_dir (:obj:`str`, `optional`):\n",
      " |          Tensorboard log directory. Will default to `runs/**CURRENT_DATETIME_HOSTNAME**`.\n",
      " |      logging_first_step (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Whether to log and evaluate the first :obj:`global_step` or not.\n",
      " |      logging_steps (:obj:`int`, `optional`, defaults to 500):\n",
      " |          Number of update steps between two logs.\n",
      " |      save_steps (:obj:`int`, `optional`, defaults to 500):\n",
      " |          Number of updates steps before two checkpoint saves.\n",
      " |      save_total_limit (:obj:`int`, `optional`):\n",
      " |          If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
      " |          :obj:`output_dir`.\n",
      " |      no_cuda (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Whether to not use CUDA even when it is available or not.\n",
      " |      seed (:obj:`int`, `optional`, defaults to 42):\n",
      " |          Random seed for initialization.\n",
      " |      fp16 (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Whether to use 16-bit (mixed) precision training (through NVIDIA apex) instead of 32-bit training.\n",
      " |      fp16_opt_level (:obj:`str`, `optional`, defaults to 'O1'):\n",
      " |          For :obj:`fp16` training, apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details\n",
      " |          on the `apex documentation <https://nvidia.github.io/apex/amp.html>`__.\n",
      " |      local_rank (:obj:`int`, `optional`, defaults to -1):\n",
      " |          During distributed training, the rank of the process.\n",
      " |      tpu_num_cores (:obj:`int`, `optional`):\n",
      " |          When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
      " |      debug (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          When training on TPU, whether to print debug metrics or not.\n",
      " |      dataloader_drop_last (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
      " |          or not.\n",
      " |      eval_steps (:obj:`int`, `optional`):\n",
      " |          Number of update steps between two evaluations if :obj:`evaluation_strategy=\"steps\"`. Will default to the\n",
      " |          same value as :obj:`logging_steps` if not set.\n",
      " |      dataloader_num_workers (:obj:`int`, `optional`, defaults to 0):\n",
      " |          Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n",
      " |          main process.\n",
      " |      past_index (:obj:`int`, `optional`, defaults to -1):\n",
      " |          Some models like :doc:`TransformerXL <../model_doc/transformerxl>` or :doc`XLNet <../model_doc/xlnet>` can\n",
      " |          make use of the past hidden states for their predictions. If this argument is set to a positive int, the\n",
      " |          ``Trainer`` will use the corresponding output (usually index 2) as the past state and feed it to the model\n",
      " |          at the next training step under the keyword argument ``mems``.\n",
      " |      run_name (:obj:`str`, `optional`):\n",
      " |          A descriptor for the run. Notably used for wandb logging.\n",
      " |      disable_tqdm (:obj:`bool`, `optional`):\n",
      " |          Whether or not to disable the tqdm progress bars. Will default to :obj:`True` if the logging level is set\n",
      " |          to warn or lower (default), :obj:`False` otherwise.\n",
      " |      remove_unused_columns (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |          If using `nlp.Dataset` datasets, whether or not to automatically remove the columns unused by the model\n",
      " |          forward method.\n",
      " |  \n",
      " |          (Note that this behavior is not implemented for :class:`~transformers.TFTrainer` yet.)\n",
      " |      label_names (:obj:`List[str]`, `optional`):\n",
      " |          The list of keys in your dictionary of inputs that correspond to the labels.\n",
      " |  \n",
      " |          Will eventually default to :obj:`[\"labels\"]` except if the model used is one of the\n",
      " |          :obj:`XxxForQuestionAnswering` in which case it will default to :obj:`[\"start_positions\",\n",
      " |          \"end_positions\"]`.\n",
      " |      load_best_model_at_end (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Whether or not to load the best model found during training at the end of training.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |              When set to :obj:`True`, the parameters :obj:`save_steps` will be ignored and the model will be saved\n",
      " |              after each evaluation.\n",
      " |      metric_for_best_model (:obj:`str`, `optional`):\n",
      " |          Use in conjunction with :obj:`load_best_model_at_end` to specify the metric to use to compare two different\n",
      " |          models. Must be the name of a metric returned by the evaluation with or without the prefix :obj:`\"eval_\"`.\n",
      " |          Will default to :obj:`\"loss\"` if unspecified and :obj:`load_best_model_at_end=True` (to use the evaluation\n",
      " |          loss).\n",
      " |  \n",
      " |          If you set this value, :obj:`greater_is_better` will default to :obj:`True`. Don't forget to set it to\n",
      " |          :obj:`False` if your metric is better when lower.\n",
      " |      greater_is_better (:obj:`bool`, `optional`):\n",
      " |          Use in conjunction with :obj:`load_best_model_at_end` and :obj:`metric_for_best_model` to specify if better\n",
      " |          models should have a greater metric or not. Will default to:\n",
      " |  \n",
      " |          - :obj:`True` if :obj:`metric_for_best_model` is set to a value that isn't :obj:`\"loss\"` or\n",
      " |            :obj:`\"eval_loss\"`.\n",
      " |          - :obj:`False` if :obj:`metric_for_best_model` is not set, or set to :obj:`\"loss\"` or :obj:`\"eval_loss\"`.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __init__(self, output_dir: str, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = None, do_predict: bool = False, evaluate_during_training: bool = False, evaluation_strategy: transformers.trainer_utils.EvaluationStrategy = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Union[int, NoneType] = None, per_gpu_eval_batch_size: Union[int, NoneType] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Union[int, NoneType] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, warmup_steps: int = 0, logging_dir: Union[str, NoneType] = <factory>, logging_first_step: bool = False, logging_steps: int = 500, save_steps: int = 500, save_total_limit: Union[int, NoneType] = None, no_cuda: bool = False, seed: int = 42, fp16: bool = False, fp16_opt_level: str = 'O1', local_rank: int = -1, tpu_num_cores: Union[int, NoneType] = None, tpu_metrics_debug: bool = False, debug: bool = False, dataloader_drop_last: bool = False, eval_steps: int = None, dataloader_num_workers: int = 0, past_index: int = -1, run_name: Union[str, NoneType] = None, disable_tqdm: Union[bool, NoneType] = None, remove_unused_columns: Union[bool, NoneType] = True, label_names: Union[List[str], NoneType] = None, load_best_model_at_end: Union[bool, NoneType] = False, metric_for_best_model: Union[str, NoneType] = None, greater_is_better: Union[bool, NoneType] = None) -> None\n",
      " |  \n",
      " |  __post_init__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  to_dict(self)\n",
      " |      Serializes this instance while replace `Enum` by their values (for JSON serialization support).\n",
      " |  \n",
      " |  to_json_string(self)\n",
      " |      Serializes this instance to a JSON string.\n",
      " |  \n",
      " |  to_sanitized_dict(self) -> Dict[str, Any]\n",
      " |      Sanitized serialization to use with TensorBoard’s hparams\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  device\n",
      " |      The device used by this process.\n",
      " |  \n",
      " |  eval_batch_size\n",
      " |      The actual batch size for evaluation (may differ from :obj:`per_gpu_eval_batch_size` in distributed training).\n",
      " |  \n",
      " |  n_gpu\n",
      " |      The number of GPUs used by this process.\n",
      " |      \n",
      " |      Note:\n",
      " |          This will only be greater than one when you have multiple GPUs available but are not using distributed\n",
      " |          training. For distributed training, it will always be 1.\n",
      " |  \n",
      " |  train_batch_size\n",
      " |      The actual batch size for training (may differ from :obj:`per_gpu_train_batch_size` in distributed training).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'adam_beta1': <class 'float'>, 'adam_beta2': <class...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'adam_beta1': Field(name='adam_beta1',type=<cl...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  adam_beta1 = 0.9\n",
      " |  \n",
      " |  adam_beta2 = 0.999\n",
      " |  \n",
      " |  adam_epsilon = 1e-08\n",
      " |  \n",
      " |  dataloader_drop_last = False\n",
      " |  \n",
      " |  dataloader_num_workers = 0\n",
      " |  \n",
      " |  debug = False\n",
      " |  \n",
      " |  disable_tqdm = None\n",
      " |  \n",
      " |  do_eval = None\n",
      " |  \n",
      " |  do_predict = False\n",
      " |  \n",
      " |  do_train = False\n",
      " |  \n",
      " |  eval_accumulation_steps = None\n",
      " |  \n",
      " |  eval_steps = None\n",
      " |  \n",
      " |  evaluate_during_training = False\n",
      " |  \n",
      " |  evaluation_strategy = 'no'\n",
      " |  \n",
      " |  fp16 = False\n",
      " |  \n",
      " |  fp16_opt_level = 'O1'\n",
      " |  \n",
      " |  gradient_accumulation_steps = 1\n",
      " |  \n",
      " |  greater_is_better = None\n",
      " |  \n",
      " |  label_names = None\n",
      " |  \n",
      " |  learning_rate = 5e-05\n",
      " |  \n",
      " |  load_best_model_at_end = False\n",
      " |  \n",
      " |  local_rank = -1\n",
      " |  \n",
      " |  logging_first_step = False\n",
      " |  \n",
      " |  logging_steps = 500\n",
      " |  \n",
      " |  max_grad_norm = 1.0\n",
      " |  \n",
      " |  max_steps = -1\n",
      " |  \n",
      " |  metric_for_best_model = None\n",
      " |  \n",
      " |  no_cuda = False\n",
      " |  \n",
      " |  num_train_epochs = 3.0\n",
      " |  \n",
      " |  overwrite_output_dir = False\n",
      " |  \n",
      " |  past_index = -1\n",
      " |  \n",
      " |  per_device_eval_batch_size = 8\n",
      " |  \n",
      " |  per_device_train_batch_size = 8\n",
      " |  \n",
      " |  per_gpu_eval_batch_size = None\n",
      " |  \n",
      " |  per_gpu_train_batch_size = None\n",
      " |  \n",
      " |  prediction_loss_only = False\n",
      " |  \n",
      " |  remove_unused_columns = True\n",
      " |  \n",
      " |  run_name = None\n",
      " |  \n",
      " |  save_steps = 500\n",
      " |  \n",
      " |  save_total_limit = None\n",
      " |  \n",
      " |  seed = 42\n",
      " |  \n",
      " |  tpu_metrics_debug = False\n",
      " |  \n",
      " |  tpu_num_cores = None\n",
      " |  \n",
      " |  warmup_steps = 0\n",
      " |  \n",
      " |  weight_decay = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TrainingArguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
